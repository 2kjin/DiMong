# AI 학습률과 모멘텀

학습률(learning rate)과 모멘텀(momentum)은 모델의 최적화(optimization) 과정에서 사용됩니다.

1. 학습률(learning rate)
   
   - 학습률은 모델이 가중치(weight)를 업데이트할 때 사용되는 스칼라 값입니다.
   
   - 학습률은 가중치 업데이트 시에 적용되는 스케일링 값으로, 학습률이 작으면 가중치가 더 조금씩 업데이트되고, 학습률이 크면 가중치가 더 많이 업데이트됩니다.
   
   - 적절한 학습률을 선택하는 것은 모델의 성능을 개선하는 데 중요합니다. 학습률이 너무 작으면 모델이 수렴하는 데 시간이 오래 걸리고, 학습률이 너무 크면 모델의 수렴이 불안정해질 수 있습니다.
- 학습률 값 설정 공식:
  
  - 학습률 값은 주로 경험적으로 설정됩니다. 보통은 0.1, 0.01, 0.001 등의 값을 사용합니다.
  
  - 학습률이 초기에 크면 학습을 빠르게 시작할 수 있지만, 모델이 수렴하기 전에 수렴 대상(converge)을 지나치게 넘어버리면서 수렴이 실패할 수 있습니다.
  
  - 일반적으로 학습률을 낮추면 안정적으로 학습을 할 수 있습니다. 학습률을 너무 낮추면 수렴 속도가 느려질 수 있으므로 적절한 값을 찾아야 합니다.
2. 모멘텀(momentum)
   
   - 모멘텀은 가중치 업데이트 시에 이전의 업데이트 방향과 크기를 고려하여 업데이트를 수행하는 방법입니다.
   
   - 모멘텀이 크면 이전 업데이트 방향과 크기가 더 많이 반영되어 현재 업데이트가 더 부드럽게 이루어집니다.
   
   - 모멘텀이 작으면 이전 업데이트 방향과 크기가 적게 반영되어 현재 업데이트가 더 예민하게 이루어집니다.
   
   - 모멘텀은 모델이 지역 최적점(local minimum)에서 벗어나는 데 도움이 됩니다. 지역 최적점에서 벗어나기 위해서는 기울기가 큰 방향으로 계속 이동해야 합니다.
-  모멘텀 값 설정 공식:
  
  - 모멘텀 값은 일반적으로 0.9, 0.95 등의 값을 사용합니다.
  - 모멘텀 값이 클수록 이전의 업데이트 방향과 크기가 더 많이 반영되어 업데이트가 부드럽게 이루어집니다.
  - 모멘텀 값이 작을수록 이전의 업데이트 방향과 크기가 적게 반영되어 업데이트가 예민하게 이루어집니다.
  - 모멘텀 값이 너무 작으면 경사하강법의 업데이트가 너무 예민해져서 수렴이 불안정해질 수 있고, 모멘텀 값이 너무 크면 경사하강법의 업데이트가 너무 부드러워져서 수렴 속도가 느려질 수 있습니다. 적절한 모멘텀 값은 경험적으로 찾아지는 경우가 많습니다.
3. 학습률 스케줄링
   
   - 학습률을 동적으로 조정하는 것은 모델 학습에서 중요한 역할을 합니다.
   
   - 학습률을 스케줄링하는 방법은 여러 가지가 있습니다. 예를 들어, `StepLR`, `MultiStepLR`, `ReduceLROnPlateau` 등의 스케줄러를 사용할 수 있습니다.
   
   ```python
   import torch.optim as optim
   from torch.optim.lr_scheduler import StepLR
   
   optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
   scheduler = StepLR(optimizer, step_size=10, gamma=0.1) # 10 에폭마다 학습률을 0.1배씩 감소
   
   for epoch in range(num_epochs):
       train(...)
       test(...)
       scheduler.step() # 에폭마다 스케줄러에 의해 학습률 조정
   
   ```
