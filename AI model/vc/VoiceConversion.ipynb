{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "epcyKQ9R_zAR",
      "metadata": {
        "id": "epcyKQ9R_zAR"
      },
      "source": [
        "# 실습: Voice Conversion 모델 동작을 위한 함수 구현\n",
        "\n",
        "본 실습의 목표는 Voice Conversion을 동작시키기 위해서 필요한 함수들을 구현하여 VC 모델을 동작시키는 것입니다. 구현이 완료된 이후에는 다양한 소스/타겟 음성을 입력하여 음성 변조 결과를 확인할 수 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GSvfU1NLAFFE",
      "metadata": {
        "id": "GSvfU1NLAFFE"
      },
      "source": [
        "### 모델 다운로드\n",
        "먼저 미리 학습되어 있는 VC 모델과, 목소리 정보를 추출할수 있는 모델을 다운로드 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ffd758b8",
      "metadata": {
        "id": "ffd758b8",
        "outputId": "e1a18150-7178-40c1-ad3e-34eafb935ab3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "CONFIG_SE_PATH = \"config_se.json\"\n",
        "CHECKPOINT_SE_PATH = \"SE_checkpoint.pth.tar\"\n",
        "# # download config \n",
        "# ! gdown --id  19cDrhZZ0PfKf2Zhr_ebB-QASRw844Tn1 -O $CONFIG_SE_PATH\n",
        "# # download checkpoint  \n",
        "# ! gdown --id   17JsW6h6TIh7-LkU2EvB_gnNrPcdBxt7X -O $CHECKPOINT_SE_PATH\n",
        "# # download checkpoint\n",
        "# ! gdown --id 1sgEjHt0lbPSEw9-FSbC_mBoOPwNi87YR -O best_model.pth.tar"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FPQpYgFIAJd1",
      "metadata": {
        "id": "FPQpYgFIAJd1"
      },
      "source": [
        "### 라이브러리 import\n",
        "필요한 라이브러리들을 import합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "398e4d9f",
      "metadata": {
        "id": "398e4d9f",
        "outputId": "ac9d5810-a49b-4a56-a596-43ea4c39f0f0",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['c:\\\\Users\\\\SSAFY\\\\Desktop\\\\SSAFY\\\\projects\\\\2special\\\\sources\\\\ai-speech-skeleton\\\\sub2\\\\SubPJT2_Voice_Conversion', 'c:\\\\Users\\\\SSAFY\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\python39.zip', 'c:\\\\Users\\\\SSAFY\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\DLLs', 'c:\\\\Users\\\\SSAFY\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib', 'c:\\\\Users\\\\SSAFY\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39', '', 'C:\\\\Users\\\\SSAFY\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages', 'C:\\\\Users\\\\SSAFY\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\win32', 'C:\\\\Users\\\\SSAFY\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\SSAFY\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\SSAFY\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages', 'C:\\\\Users\\\\SSAFY\\\\Desktop\\\\SSAFY\\\\projects\\\\2special\\\\sources\\\\ai-speech-skeleton\\\\sub2\\\\SubPJT2_Voice_Conversion\\\\TTS']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "TTS_PATH = \"C:\\\\Users\\\\SSAFY\\\\Desktop\\\\SSAFY\\\\projects\\\\2special\\\\sources\\\\ai-speech-skeleton\\\\sub2\\\\SubPJT2_Voice_Conversion\\\\TTS\"\n",
        "\n",
        "# add libraries into environment\n",
        "sys.path.append(TTS_PATH) # set this if TTS is not installed globally\n",
        "print(sys.path)\n",
        "import os\n",
        "import string\n",
        "import time\n",
        "import argparse\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "from TTS.tts.utils.synthesis import synthesis\n",
        "from TTS.tts.utils.text.symbols import make_symbols, phonemes, symbols\n",
        "try:\n",
        "  from TTS.utils.audio import AudioProcessor\n",
        "except:\n",
        "  from TTS.utils.audio import AudioProcessor\n",
        "\n",
        "\n",
        "from TTS.tts.models import setup_model\n",
        "from TTS.config import load_config\n",
        "from TTS.tts.models.vits import *\n",
        "\n",
        "from TTS.tts.utils.speakers import SpeakerManager\n",
        "from pydub import AudioSegment\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lNsyr8DhAiJx",
      "metadata": {
        "id": "lNsyr8DhAiJx"
      },
      "source": [
        "### Voice Conversion 모델 세팅\n",
        "미리 학습된 Voice Conversion 모델을 동작하기 위한 기본적인 세팅을 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6768361e",
      "metadata": {
        "id": "6768361e",
        "outputId": "8c423998-7bb8-4ba7-dd97-ae82eabe483b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > Setting up Audio Processor...\n",
            " | > sample_rate:16000\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:1024\n",
            " | > power:1.5\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:True\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > spec_gain:1.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:True\n",
            " | > trim_db:45\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:False\n",
            " | > do_amp_to_db_mel:True\n",
            " | > stats_path:None\n",
            " | > base:2.718281828459045\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Using model: vits\n",
            " > Speaker manager is loaded with 6 speakers: female-en-5, female-en-5\n",
            ", female-pt-4\n",
            ", male-en-2, male-en-2\n",
            ", male-pt-3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# model vars \n",
        "MODEL_PATH = 'best_model.pth.tar'\n",
        "CONFIG_PATH = 'config.json'\n",
        "TTS_LANGUAGES = \"language_ids.json\"\n",
        "TTS_SPEAKERS = \"speakers.json\"\n",
        "SAMPLING_RATE=16000\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "# load the config\n",
        "C = load_config(CONFIG_PATH)\n",
        "# load the audio processor\n",
        "ap = AudioProcessor(**C.audio)\n",
        "\n",
        "speaker_embedding = None\n",
        "\n",
        "C.model_args['d_vector_file'] = TTS_SPEAKERS\n",
        "C.model_args['use_speaker_encoder_as_loss'] = False\n",
        "\n",
        "model = setup_model(C)\n",
        "model.language_manager.set_language_ids_from_file(TTS_LANGUAGES)\n",
        "# print(model.language_manager.num_languages, model.embedded_language_dim)\n",
        "# print(model.emb_l)\n",
        "cp = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
        "# remove speaker encoder\n",
        "model_weights = cp['model'].copy()\n",
        "for key in list(model_weights.keys()):\n",
        "  if \"speaker_encoder\" in key:\n",
        "    del model_weights[key]\n",
        "\n",
        "model.load_state_dict(model_weights)\n",
        "model.eval()\n",
        "\n",
        "if USE_CUDA:\n",
        "    model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ALCEVgpIA02U",
      "metadata": {
        "id": "ALCEVgpIA02U"
      },
      "source": [
        "### Speaker Encoder 모델 세팅\n",
        "미리 학습된 Speaker Encoder 모델을 동작하기 위한 기본적인 세팅을 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "00cc35ed",
      "metadata": {
        "id": "00cc35ed",
        "outputId": "8c7b11f2-df53-4b00-b703-88eaf8bc52df",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > Setting up Audio Processor...\n",
            " | > sample_rate:16000\n",
            " | > resample:False\n",
            " | > num_mels:64\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:512\n",
            " | > power:1.5\n",
            " | > preemphasis:0.97\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:False\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:8000.0\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:False\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:160\n",
            " | > win_length:400\n"
          ]
        }
      ],
      "source": [
        "SE_speaker_manager = SpeakerManager(encoder_model_path=CHECKPOINT_SE_PATH, encoder_config_path=CONFIG_SE_PATH, use_cuda=USE_CUDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UK8bR36FBPJK",
      "metadata": {
        "id": "UK8bR36FBPJK"
      },
      "source": [
        "#실습 진행하기\n",
        "\n",
        "## Req. 2-2:\tSpectrogram을 생성하는 compute_spec() 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d34f78d4",
      "metadata": {
        "id": "d34f78d4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "\n",
        "def compute_spec(ref_file):\n",
        "  N_FFT = 1024\n",
        "  SR = 16000\n",
        "  # print(ref_file)\n",
        "  # sig,sr =librosa.load(ref_file,sr=SR)\n",
        "  # stft = librosa.stft(y=sig,n_fft=N_FFT,hop_length=256,pad_mode=\"reflect\", window=\"hann\")\n",
        "  # magnitude = np.abs(stft)\n",
        "  # mel_basis = librosa_mel_fn(SR,n_fft=N_FFT,n_mels=128)\n",
        "  # mel = np.dot(a=mel_basis,b=magnitude)\n",
        "  # mel = 20 * np.log10(np.maximum(1e-5, mel))\n",
        "\n",
        "  # # normalize\n",
        "  # mel = np.clip((mel - 20 + 100) / 100, 1e-8, 1)\n",
        "  # mel = np.float32(mel)\n",
        "  # mel = torch.from_numpy(mel)\n",
        "  # return mel[None,:]\n",
        "  \n",
        "  y, sr = librosa.load(path=ref_file, sr=SR)\n",
        "  spec = np.abs(librosa.stft(y=y, n_fft=1024, hop_length=256,win_length=1024, pad_mode=\"reflect\", window=\"hann\"))\n",
        "  spec = np.float32(spec)\n",
        "  spec = torch.from_numpy(spec)\n",
        "  # y, sr = librosa.load(path=ref_file, sr=SR)\n",
        "  # stft=librosa.stft(y=y, n_fft=1024, hop_length=256,win_length=1024, pad_mode=\"reflect\", window=\"hann\")\n",
        "  # magnitude = np.abs(stft)\n",
        "  # mel_basis = librosa_mel_fn(SR,n_fft=N_FFT,n_mels=513)\n",
        "  # mel = np.dot(a=mel_basis,b=magnitude)\n",
        "  # spec = np.abs(mel)\n",
        "  # spec = np.float32(spec)\n",
        "  # spec = torch.from_numpy(spec)\n",
        "  return spec[None,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5BNp8v-iBRZM",
      "metadata": {
        "id": "5BNp8v-iBRZM"
      },
      "source": [
        "### Req. 2-2의 구현을 완료한 뒤 테스트 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "31f8ba7a",
      "metadata": {
        "id": "31f8ba7a",
        "outputId": "6b123ffb-0487-4e68-8042-36f9d3f663ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of the test spectrogram:  torch.Size([1, 513, 376])\n",
            "max value of the test spectrogram:  tensor(21.7452)\n",
            "min value of the test spectrogram:  tensor(0.)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\librosa\\filters.py:238: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "####Req. 2-2 test 용도####\n",
        "test_audio = \"./jupyter/source/test.wav\"\n",
        "test_spec = compute_spec(test_audio)\n",
        "print(\"shape of the test spectrogram: \", test_spec.shape)\n",
        "print(\"max value of the test spectrogram: \", test_spec.max())\n",
        "print(\"min value of the test spectrogram: \", test_spec.min())\n",
        "####Req. 2-2 test 용도####"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GNoyBYfSBcMM",
      "metadata": {
        "id": "GNoyBYfSBcMM"
      },
      "source": [
        "### Req. 2-3의 구현을 완료한 뒤 테스트 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d52afc0f",
      "metadata": {
        "id": "d52afc0f",
        "outputId": "348cc4f8-8c25-497a-f58f-1c3048695684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of the test embedding:  torch.Size([1, 512])\n",
            "max value of the test embedding:  tensor(0.2351, device='cuda:0')\n",
            "min value of the test embedding:  tensor(-0.2167, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "####Req. 2-3 test 용도####\n",
        "test_audio = \"./jupyter/source/test.wav\" \n",
        "test_emb = SE_speaker_manager.compute_speaker_embedding(test_audio)\n",
        "print(\"shape of the test embedding: \", test_emb.shape)\n",
        "print(\"max value of the test embedding: \", test_emb.max())\n",
        "print(\"min value of the test embedding: \", test_emb.min())\n",
        "####Req. 2-3 test 용도####"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GQJQudrhBfIQ",
      "metadata": {
        "id": "GQJQudrhBfIQ"
      },
      "source": [
        "### Req. 2-4의 구현을 완료한 뒤 테스트 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1753e3af",
      "metadata": {
        "id": "1753e3af",
        "outputId": "7c82ff7a-491c-49d6-8cff-b4f833c077e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max value of the test embedding:  tensor(0.1433, device='cuda:0')\n",
            "min value of the test embedding:  tensor(-0.2144, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "####Req. 2-4 test 용도####\n",
        "test_audios = [\"./jupyter/source/test.wav\", \"./jupyter/source/test2.wav\"]\n",
        "test_emb = SE_speaker_manager.compute_d_vector_from_clip(test_audios)\n",
        "print(\"max value of the test embedding: \", test_emb.max())\n",
        "print(\"min value of the test embedding: \", test_emb.min())\n",
        "####Req. 2-4 test 용도####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "98008518",
      "metadata": {
        "id": "98008518",
        "outputId": "926c51c3-2ff5-4c8e-cc3e-8959aa9022f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max value of the test embedding:  tensor(0.1345, device='cuda:0')\n",
            "min value of the test embedding:  tensor(-0.2572, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "####Req. 2-4 test 용도####\n",
        "test_audios = [\"./jupyter/source/test2.wav\"]\n",
        "test_emb = SE_speaker_manager.compute_d_vector_from_clip(test_audios)\n",
        "print(\"max value of the test embedding: \", test_emb.max())\n",
        "print(\"min value of the test embedding: \", test_emb.min())\n",
        "####Req. 2-4 test 용도####"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o7-JhmHzBtP4",
      "metadata": {
        "id": "o7-JhmHzBtP4"
      },
      "source": [
        "# Voice Conversion 모델을 동작합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4dccda69",
      "metadata": {
        "id": "4dccda69",
        "outputId": "f9c1d3a3-ab8a-45e2-ed0b-a7777cbc33c7",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Select target speaker reference audios files:\n"
          ]
        }
      ],
      "source": [
        "print(\"Select target speaker reference audios files:\")\n",
        "target_files = \"./jupyter/source/test2.wav\"\n",
        "target_files = [target_files]\n",
        "target_emb = SE_speaker_manager.compute_d_vector_from_clip(target_files)\n",
        "# target_emb = torch.FloatTensor(target_emb).unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a86d28c5",
      "metadata": {
        "id": "a86d28c5",
        "outputId": "b04dfade-b753-4e40-df12-cde7f54d61f2",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Select driving audio file:\n"
          ]
        }
      ],
      "source": [
        "print(\"Select driving audio file:\")\n",
        "driving_file = \"./jupyter/source/Ankylosaurus.wav\"\n",
        "driving_file = [driving_file]\n",
        "driving_emb = SE_speaker_manager.compute_d_vector_from_clip(driving_file)\n",
        "# driving_emb = torch.FloatTensor(driving_emb).unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UTgMisQvB0fc",
      "metadata": {
        "id": "UTgMisQvB0fc"
      },
      "source": [
        "## Req. 2-5:\t소스 음성과 타켓 음성의 embedding을 추출하는 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fd9fac00",
      "metadata": {
        "id": "fd9fac00",
        "outputId": "eaedb5b4-ffe7-4be5-cda7-e986c4b00a22",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def get_embeddings():\n",
        "    pass\n",
        "################################################################################\n",
        "# TODO: 소스 음성과 타켓 음성의 embedding을 추출하는 함수 구현                            #\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9ea5b7ff",
      "metadata": {
        "id": "9ea5b7ff",
        "outputId": "a56e0d61-b3fd-4aff-d6ee-f9960ead4b8c",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./jupyter/source/Ankylosaurus.wav\n",
            "./jupyter/source/Ankylosaurus.wav\n",
            "torch.Size([1, 128, 1459])\n",
            "tensor([1459])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [192, 513, 1], expected input[1, 128, 1459] to have 513 channels, but got 128 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(y_lengths)\n\u001b[0;32m      8\u001b[0m \u001b[39mif\u001b[39;00m USE_CUDA:\n\u001b[1;32m----> 9\u001b[0m     ref_wav_voc, _, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mvoice_conversion(driving_spec\u001b[39m.\u001b[39;49mcuda(), y_lengths\u001b[39m.\u001b[39;49mcuda(), driving_emb\u001b[39m.\u001b[39;49mcuda(), target_emb\u001b[39m.\u001b[39;49mcuda())\n\u001b[0;32m     10\u001b[0m     ref_wav_voc \u001b[39m=\u001b[39m ref_wav_voc\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     11\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\SSAFY\\Desktop\\SSAFY\\projects\\2special\\sources\\ai-speech-skeleton\\sub2\\SubPJT2_Voice_Conversion\\TTS\\tts\\models\\vits.py:786\u001b[0m, in \u001b[0;36mVits.voice_conversion\u001b[1;34m(self, y, y_lengths, speaker_cond_src, speaker_cond_tgt)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    784\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m [!] Voice conversion is only supported on multi-speaker models.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 786\u001b[0m z, _, _, y_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposterior_encoder(y, y_lengths, g\u001b[39m=\u001b[39;49mg_src)\n\u001b[0;32m    787\u001b[0m z_p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflow(z, y_mask, g\u001b[39m=\u001b[39mg_src)\n\u001b[0;32m    788\u001b[0m z_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflow(z_p, y_mask, g\u001b[39m=\u001b[39mg_tgt, reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\SSAFY\\Desktop\\SSAFY\\projects\\2special\\sources\\ai-speech-skeleton\\sub2\\SubPJT2_Voice_Conversion\\TTS\\tts\\layers\\vits\\networks.py:276\u001b[0m, in \u001b[0;36mPosteriorEncoder.forward\u001b[1;34m(self, x, x_lengths, g)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39mShapes:\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m    - x: :math:`[B, C, T]`\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39m    - x_lengths: :math:`[B, 1]`\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[39m    - g: :math:`[B, C, 1]`\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    275\u001b[0m x_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(sequence_mask(x_lengths, x\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m)), \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m--> 276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre(x) \u001b[39m*\u001b[39m x_mask\n\u001b[0;32m    277\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc(x, x_mask, g\u001b[39m=\u001b[39mg)\n\u001b[0;32m    278\u001b[0m stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(x) \u001b[39m*\u001b[39m x_mask\n",
            "File \u001b[1;32mc:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[1;32mc:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [192, 513, 1], expected input[1, 128, 1459] to have 513 channels, but got 128 channels instead"
          ]
        }
      ],
      "source": [
        "print(driving_file[0])\n",
        "driving_spec = compute_spec(driving_file[0])\n",
        "print(driving_spec.cuda().shape)\n",
        "y_lengths = torch.tensor([driving_spec.size(-1)])\n",
        "\n",
        "print(y_lengths)\n",
        "\n",
        "if USE_CUDA:\n",
        "    ref_wav_voc, _, _ = model.voice_conversion(driving_spec.cuda(), y_lengths.cuda(), driving_emb.cuda(), target_emb.cuda())\n",
        "    ref_wav_voc = ref_wav_voc.squeeze().cpu().detach().numpy()\n",
        "else:\n",
        "    ref_wav_voc, _, _ = model.voice_conversion(driving_spec, y_lengths, driving_emb, target_emb)\n",
        "    ref_wav_voc = ref_wav_voc.squeeze().detach().numpy()\n",
        "\n",
        "\n",
        "print(\"Target Speaker reference Audio\")\n",
        "IPython.display.display(Audio(target_files[0], rate=ap.sample_rate))\n",
        "\n",
        "print(\"Source speaker reference Audio\")\n",
        "IPython.display.display(Audio(driving_file[0], rate=ap.sample_rate))\n",
        "\n",
        "print(\"Play the converted audio:\")\n",
        "IPython.display.display(Audio(ref_wav_voc, rate=SAMPLING_RATE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe25334",
      "metadata": {
        "id": "1fe25334"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
